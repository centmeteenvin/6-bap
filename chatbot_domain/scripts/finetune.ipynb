{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\Documents\\school\\Unief-6\\6-bap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 03:27:48,222 - chatbot_domain - INFO - loading model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d35a8177cc47f2a8abebdc31e42495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae14c7d1ed4f408ab9a7589a1cd46944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 03:28:20,348 - chatbot_domain - INFO - Reading pdf-file\n",
      "2024-04-11 03:28:31,988 - chatbot_domain - INFO - Finished reading pdf.\n",
      "Extracted 1641 paragraphs and 15728 sentences\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m dataSet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataSetLoadPath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m     sentences, alineas \u001b[38;5;241m=\u001b[39m parseData(dataFilePath)\n\u001b[0;32m     37\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(alineas)\n\u001b[0;32m     38\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(sentences)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "%cd ../../\n",
    "%pwd\n",
    "\n",
    "from chatbot_domain.chatbot.chatbot import ChatBot\n",
    "from chatbot_domain.data import createDataSet, loadDataSetDictFromDisk, parseData\n",
    "from chatbot_domain.finetune.train import Training\n",
    "from chatbot_domain.settings import Settings\n",
    "from chatbot_domain.transformers.model import Model\n",
    "from chatbot_domain import logger\n",
    "from chatbot_domain.transformers.tokenizer import Tokenizer\n",
    "\n",
    "Settings.modelName = 'google/gemma-2b'\n",
    "from chatbot_domain.secrets import HUGGINGFACE_ACCESS\n",
    "Settings.accesToken = HUGGINGFACE_ACCESS\n",
    "Settings.trainingOutput = Settings.modelName + \"/training\"\n",
    "Settings.batchSize = 32\n",
    "Settings.epochs = 3\n",
    "Settings.evalBatchSizeMultiplier = 2\n",
    "Settings.gradientAccumulatorMultiplier = 2\n",
    "Settings.shouldTrain = True\n",
    "Settings.modelPath = None #Settings.modelName + \"/model\"\n",
    "Settings.shouldQuantize = False\n",
    "Settings.autoMap = False\n",
    "Settings.optimizer = 'adamw_torch_fused'\n",
    "Settings.shouldChat = False\n",
    "Settings.modelSavePath = Settings.modelName + \"/model\"\n",
    "\n",
    "dataSetLoadPath = None\n",
    "dataFilePath = './DIP-TB.pdf'\n",
    "dataSetStorePath = './trainData.set'\n",
    "\n",
    "model = Model()\n",
    "if Settings.shouldTrain:\n",
    "    dataSet = None\n",
    "    if dataSetLoadPath is None:\n",
    "        data = parseData(dataFilePath)\n",
    "        dataSet = createDataSet(data, 0.05)\n",
    "        dataSet = Tokenizer.get().encode(dataSet)\n",
    "        if dataSetStorePath:\n",
    "            logger.info(f\"Storing dataset to {dataSetStorePath}\")\n",
    "            dataSet.save_to_disk(dataSetStorePath)\n",
    "    else:\n",
    "        logger.info(\"Loading dataset from disk\")\n",
    "        dataSet = loadDataSetDictFromDisk(dataSetLoadPath)\n",
    "    logger.info(f\"dataset: {dataSet}\")\n",
    "    trainer = Training(model, dataSet)\n",
    "    trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
