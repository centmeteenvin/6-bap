{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\Documents\\school\\Unief-6\\6-bap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 13:32:16,064 - chatbot_domain - INFO - loading model\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f62c1d817fa40eb914bfca8e6ce2825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "2024-04-14 13:32:28,888 - chatbot_domain - INFO - Loading tokenizer\n",
      "INFO:chatbot_domain:Loading tokenizer\n",
      "c:\\Python311\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:573: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "gemma-2b-it : Hello! It's great to hear from you. How can I assist you today?\n",
      "================================================================================\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "%cd ../../\n",
    "%pwd\n",
    "\n",
    "from chatbot_domain.chatbot.chatbot import ChatBot\n",
    "from chatbot_domain.data import createDataSet, loadDataSetDictFromDisk, parseData\n",
    "from chatbot_domain.finetune.train import Training\n",
    "from chatbot_domain.settings import Settings\n",
    "from chatbot_domain.transformers.model import Model\n",
    "from chatbot_domain import logger\n",
    "from chatbot_domain.transformers.tokenizer import Tokenizer\n",
    "from chatbot_domain.chatbot import ModelChatbot\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['NCCL_P2P_DISABLE'] = '1'\n",
    "os.environ['NCCL_IB_DISABLE'] = '1'\n",
    "\n",
    "Settings.modelName = 'google/gemma-2b-it'\n",
    "from chatbot_domain.secrets import HUGGINGFACE_ACCESS\n",
    "Settings.accesToken = HUGGINGFACE_ACCESS\n",
    "Settings.trainingOutput = 'models/' + Settings.modelName + \"/training\"\n",
    "Settings.batchSize = 1\n",
    "Settings.epochs = 3\n",
    "Settings.evalBatchSizeMultiplier = 2\n",
    "Settings.gradientAccumulatorMultiplier = 1\n",
    "Settings.shouldTrain = True\n",
    "Settings.modelPath = None #Settings.modelName + \"/model\"\n",
    "Settings.shouldQuantize = False\n",
    "Settings.autoMap = False\n",
    "Settings.optimizer = 'adamw_torch_fused'\n",
    "Settings.shouldChat = False\n",
    "Settings.modelSavePath = 'models/' + Settings.modelName + \"/model\"\n",
    "\n",
    "dataSetLoadPath = None\n",
    "dataFilePath = './DIP-TB.pdf'\n",
    "dataSetStorePath = './trainData.set'\n",
    "\n",
    "model = Model()\n",
    "if Settings.shouldTrain:\n",
    "    dataSet = None\n",
    "    if dataSetLoadPath is None:\n",
    "        data = parseData(dataFilePath)\n",
    "        dataSet = createDataSet(data, 0.05)\n",
    "        dataSet = Tokenizer.get().encode(dataSet)\n",
    "        if dataSetStorePath:\n",
    "            logger.info(f\"Storing dataset to {dataSetStorePath}\")\n",
    "            dataSet.save_to_disk(dataSetStorePath)\n",
    "    else:\n",
    "        logger.info(\"Loading dataset from disk\")\n",
    "        dataSet = loadDataSetDictFromDisk(dataSetLoadPath)\n",
    "    logger.info(f\"dataset: {dataSet}\")\n",
    "    trainer = Training(model, dataSet)\n",
    "    trainer.train()\n",
    "\n",
    "chatbot : ModelChatbot = ModelChatbot(model=model, tokenizer=Tokenizer.get())\n",
    "chatbot.startConversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from chatbot_domain.chatbot.chatbot import ChatBot\n",
    "from chatbot_domain.data import createDataSet, loadDataSetDictFromDisk, parseData\n",
    "from chatbot_domain.finetune.train import Training\n",
    "from chatbot_domain.settings import Settings\n",
    "from chatbot_domain.transformers.model import Model\n",
    "from chatbot_domain import logger\n",
    "from chatbot_domain.transformers.tokenizer import Tokenizer\n",
    "from chatbot_domain.chatbot import ModelChatbot\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['NCCL_P2P_DISABLE'] = '1'\n",
    "os.environ['NCCL_IB_DISABLE'] = '1'\n",
    "\n",
    "Settings.modelName = 'google/gemma-2b-it'\n",
    "from chatbot_domain.secrets import HUGGINGFACE_ACCESS\n",
    "Settings.accesToken = HUGGINGFACE_ACCESS\n",
    "Settings.trainingOutput = 'models/' + Settings.modelName + \"/training\"\n",
    "Settings.batchSize = 1\n",
    "Settings.epochs = 3\n",
    "Settings.evalBatchSizeMultiplier = 2\n",
    "Settings.gradientAccumulatorMultiplier = 1\n",
    "Settings.shouldTrain = True\n",
    "Settings.modelPath = None #Settings.modelName + \"/model\"\n",
    "Settings.shouldQuantize = False\n",
    "Settings.autoMap = False\n",
    "Settings.optimizer = 'adamw_torch_fused'\n",
    "Settings.shouldChat = False\n",
    "Settings.modelSavePath = 'models/' + Settings.modelName + \"/model\"\n",
    "\n",
    "dataSetLoadPath = None\n",
    "dataFilePath = './DIP-TB.pdf'\n",
    "dataSetStorePath = './trainData.set'\n",
    "\n",
    "model = Model()\n",
    "if Settings.shouldTrain:\n",
    "    dataSet = None\n",
    "    if dataSetLoadPath is None:\n",
    "        data = parseData(dataFilePath)\n",
    "        dataSet = createDataSet(data, 0.05)\n",
    "        dataSet = Tokenizer.get().encode(dataSet)\n",
    "        if dataSetStorePath:\n",
    "            logger.info(f\"Storing dataset to {dataSetStorePath}\")\n",
    "            dataSet.save_to_disk(dataSetStorePath)\n",
    "    else:\n",
    "        logger.info(\"Loading dataset from disk\")\n",
    "        dataSet = loadDataSetDictFromDisk(dataSetLoadPath)\n",
    "    logger.info(f\"dataset: {dataSet}\")\n",
    "    trainer = Training(model, dataSet)\n",
    "    trainer.train()\n",
    "\n",
    "chatbot : ModelChatbot = ModelChatbot(model=model, tokenizer=Tokenizer.get())\n",
    "chatbot.startConversation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
