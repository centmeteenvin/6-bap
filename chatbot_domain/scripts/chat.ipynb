{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdc34acb-bcf0-4c2f-929b-651a109d7923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\Documents\\school\\Unief-6\\6-bap\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\vince\\\\Documents\\\\school\\\\Unief-6\\\\6-bap'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ../../\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b329345-735b-4b2f-8009-325317ea28ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 10:25:15,010 - chatbot_domain - INFO - loading model\n",
      "2024-04-23 10:25:15,010 - chatbot_domain - INFO - With Quantization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\Documents\\school\\Unief-6\\6-bap\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eefbd34774d4e939b4476b4af551bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/737 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3efc12a4c94465f8d33628625b3e2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m      9\u001b[0m logger\u001b[38;5;241m.\u001b[39msetLevel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDEBUG\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# dpr = FacebookDPR()\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# dataset = loadDataSetFromDisk('./ragData.set')\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# retriever = VectorRetriever(dataset=dataset, dpr=dpr, datasetLocation='./ragData.Set')\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# chatbot = ChatBotModifier(OpenAIChatBot()).rag(retriever, 2048).domainGuard(DIPDomainGuard).build()\u001b[39;00m\n\u001b[0;32m     14\u001b[0m chatbot \u001b[38;5;241m=\u001b[39m ChatBotBuilder\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mModelBuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeviceMap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodelName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvincentverbergt/autotrain-u5y0w-pa1vc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshouldQuantize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     )\u001b[38;5;241m.\u001b[39mbuild()\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\school\\Unief-6\\6-bap\\chatbot_domain\\transformers\\builder.py:23\u001b[0m, in \u001b[0;36mModelBuilder.build\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m\u001b[38;5;28mtuple\u001b[39m[Model, Tokenizer]:\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modelName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shouldQuantize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deviceMap\u001b[49m\u001b[43m)\u001b[49m, Tokenizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modelName)\n",
      "File \u001b[1;32mc:\\Users\\vince\\Documents\\school\\Unief-6\\6-bap\\chatbot_domain\\transformers\\model.py:21\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, modelName, shouldQuantize, deviceMap)\u001b[0m\n\u001b[0;32m     13\u001b[0m     bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[0;32m     14\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mcompute_dtype,\n\u001b[0;32m     17\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     18\u001b[0m     )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchatbot_domain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msecrets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HUGGINGFACE_ACCESS\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel : AutoModelForCausalLM\u001b[38;5;241m=\u001b[39m  \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodelName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mHUGGINGFACE_ACCESS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdeviceMap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:3531\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3523\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   3524\u001b[0m     (\n\u001b[0;32m   3525\u001b[0m         model,\n\u001b[0;32m   3526\u001b[0m         missing_keys,\n\u001b[0;32m   3527\u001b[0m         unexpected_keys,\n\u001b[0;32m   3528\u001b[0m         mismatched_keys,\n\u001b[0;32m   3529\u001b[0m         offload_index,\n\u001b[0;32m   3530\u001b[0m         error_msgs,\n\u001b[1;32m-> 3531\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[0;32m   3535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3538\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3539\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3542\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3543\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3547\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3549\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[0;32m   3550\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:3958\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[0;32m   3954\u001b[0m                 set_module_tensor_to_device(\n\u001b[0;32m   3955\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   3956\u001b[0m                 )\n\u001b[0;32m   3957\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3958\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3959\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3964\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3965\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3966\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3967\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3975\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[0;32m   3976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:764\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[1;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[0;32m    762\u001b[0m             set_module_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m    763\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 764\u001b[0m         param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;66;03m# For compatibility with PyTorch load_state_dict which converts state dict dtype to existing dtype in model, and which\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;66;03m# uses `param.copy_(input_param)` that preserves the contiguity of the parameter in the model.\u001b[39;00m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;66;03m# Reference: https://github.com/pytorch/pytorch/blob/db79ceb110f6646523019a59bbd7b838f43d4a86/torch/nn/modules/module.py#L2040C29-L2040C29\u001b[39;00m\n\u001b[0;32m    769\u001b[0m old_param \u001b[38;5;241m=\u001b[39m model\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from chatbot_domain.chatbot import *\n",
    "from chatbot_domain.transformers import ModelBuilder\n",
    "from chatbot_domain.rag import VectorRetriever, FacebookDPR\n",
    "from chatbot_domain import logger\n",
    "from chatbot_domain.data import loadDataSetFromDisk\n",
    "\n",
    "from os import getcwd\n",
    "print(getcwd())\n",
    "logger.setLevel('DEBUG')\n",
    "# dpr = FacebookDPR()\n",
    "# dataset = loadDataSetFromDisk('./ragData.set')\n",
    "# retriever = VectorRetriever(dataset=dataset, dpr=dpr, datasetLocation='./ragData.Set')\n",
    "# chatbot = ChatBotModifier(OpenAIChatBot()).rag(retriever, 2048).domainGuard(DIPDomainGuard).build()\n",
    "chatbot = ChatBotBuilder.model(\n",
    "    ModelBuilder().deviceMap(\"auto\").modelName(\"vincentverbergt/Mistral7B-DIP\").shouldQuantize(True).build(adapted=True)\n",
    "    ).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba00a44c-7b84-4067-8119-2be874bd2384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 08:22:02,588 - chatbot_domain - DEBUG - The final prompt is What is Canny Edge detection?\n",
      "2024-04-19 08:22:02,588 - chatbot_domain - DEBUG - The added role was You are an assistant with expertise in the Image Processing domain. Your knowledge is given through the 'CONTEXT' section. You will no answer any questions that are not related to the image processing domain. If you are not confident in your answer you will tell us so. If the question is not related to the domain you will answer with 'This question is outside my domain of knowledge'\n",
      "2024-04-19 08:22:02,641 - chatbot_domain - DEBUG - Context concatenation loop completed without breaking, updating sampleGuess to 20\n",
      "2024-04-19 08:22:02,641 - chatbot_domain - DEBUG - rerunning context gathering with higher samples\n",
      "2024-04-19 08:22:02,699 - chatbot_domain - DEBUG - Context concatenation loop completed without breaking, updating sampleGuess to 40\n",
      "2024-04-19 08:22:02,699 - chatbot_domain - DEBUG - rerunning context gathering with higher samples\n",
      "2024-04-19 08:22:02,768 - chatbot_domain - DEBUG - Had to cutoff the context due to the context window limit. updating sampleGuess to 36\n",
      "2024-04-19 08:22:02,768 - chatbot_domain - DEBUG - The added context was \n",
      "Alternative definition An alternative definition of the opening will allow us to determine the opening in a single step process.\n",
      "8.4.4.2 Regional analysis  piecewise linear edge modeling We will restrict ourselves to one particular regional method i.e. creating a polylinear model based on an ordered set of edge pixels. A second prerequisite is that we need to know if the edge is a closed or an open path. If it is an open path we can apply the algorithm straight away. If it a closed path we need to split it into two open half paths.\n",
      "6.3. COLOR 119 .1 XYZ 380410430460 490 520 550 580 610640 Figure 6.12 The trichromatic normalization plane added to the monochromatic trajectory graph of Fig ure6.11 on page 117 perceptions as any of these points can be created by a weighted combination of pure monochromatic colors. This is based on the assumption that our vision is linear with respect to color perception. Points outside of the color gamut dont correspond to tristimulus signals that will ever occur in a normal brain.\n",
      "Alternative definition An alternative definition of the closing will allow us to determine the closing in a single step process.\n",
      "If we categorize electromagnetic waves in vacuum according to their frequency or wavelength we obtain the spectrum of Figure 6.1. Light is only a small subset of that entire spectrum. Visible light ranges from 400 nm violet to 700 nm red. The neighbors of that range ultra violet below 400 nm and infra red above 700 nm are often also considered to be non visible light. It might be obvious to you that we use colors to name the different wavelengths. However if youre stupefied by that fact bear a little longer it will become clear when discussing the human eye in section 6.2 on page 112 .\n",
      "By discretizing images we transform them to the domain where digital computers are at their best number crunching.\n",
      "4.3 Resampling 4.3.1 The need for resampling Given the fact that we have a sampled version of the image we want to consider one might wonder why do we need resampling?\n",
      "In that sense this is the first chapter in which we will perform image analysis rather than image processing . Remember the former means determining characteristics of an image while the latter means processing one image into another see section 3.3 on page 20 .\n",
      "Either the objective is to manipulate the incoming images to produce new images and display them to a user. In that case we proceed further to the right converting our digital images again to the analog domain using a DAC converter and an amplifier \u001cto drive the image actuatorthat displays the resulting image as seen by our eye. Very often these last parts starting from the DAC are integrated into a standalone image generation system e.g. an LCD monitor.\n",
      "7.3.5.4 Convex hull determination In many cases processing a full image with complex imaging algorithms requires too much computing power. Determination of a region of interest is required to allow focusing the image analysis onto a specific part of the image. Usually the shape of the hull around the object of interest should not be overly complex. Demanding it to be rectangular or diamondshaped might seem appropriate.\n",
      "The iterative application of the geodesic dilation converges to a stable image.2Applying it until convergence occurs is the socalled reconstruction by dilation and is denoted as   withsuch that1  .\n",
      "The geodesic operations will enable us to fully reconstruct parts of an image.\n",
      "The concepts of translation and reflection have been illustrated in Figure 7.4.\n",
      "We will treat three methods a local one a regional one and a global one. While the local method is not a very good one  we added it for completeness  the latter two are good performers.\n",
      "A.1 Experiments and outcomes The very basic concept of stochastic theory is the experiment . Consider it to be a particular action that one can take repeatedly  and leads to a specific outcome a result.\n",
      "The principle is to subtract an eroded version of the image from a dilated version of the image.\n",
      "A.2 Events An event is a subset of the outcome set. If the outcome of an experiment is part of this subset then we say theeventhappened . If it is not a part of the event subset then we say theeventdidnot happen.\n",
      "In the former case we samplea continuous image to obtain an image that consists of a grid of pixels. This a physical process. In the latter case we resample a discretedomain image to obtain a new image with a new grid of pixels. The need for this resampling is dictated by our desire to look at images from a different point of view requiring domaintransformations . This is a mathematical process.\n",
      "For that case the issue has been illustrated in Figure 4.10 on the next page . Subfigure a shows a discretetime signal that wed like to resample. The desired offgrid new sample locations have been indicated in subfigure b. The key question is how do we find sensible DIP20233.9TB Digital Image Processing  Text book\n",
      "Example As an example consider the histogram data below. It belongs to two simple 1111 test images and quantized using 3bitintensity values . Imageagain corresponds to Figure5.3 on page 70 . Imageis in fact nonexistent. The histogram is just a linear decreasing histogram for testing purposes.\n",
      "GH 1 GH 4.4.4 Common geometric transformations In this section we will discuss a number of common geometric transformations. We will limit ourselves to transformations that maintain lines. We will consider Linear transformations Affine transformations Projective transformations The former two maintain the parallelism of lines. The latter does not. Instead it allows to correct or invoke perspective in the image.\n",
      "One could consider imaging systems and image synthesis systems to be flavors of the same kind. We make the difference to stress the fact that image synthesis systems not necessarily strive to generate images that correspond to a physical reality wheras imaging systems do.\n",
      "Thegeodesic operations tackle this issue by adding a masking operation to the basic operations to correct for the boldness or the specific shape of the brush. To continue on the painters metaphor it is like using masking tape when painting a decoration on a wall the masking tape avoids painting areas that should not carry any paint. It diminishes the influence of the brush on the end result.\n",
      "186 CHAPTER 7. MATHEMATICALMORPHOLOGY As an example consider the image in Figure 7.27a. We indicated a small rectangular cutout on the image that we will study in detail. The cutout is taken from the radiator grille of the toy car on the left. A magnified version of the cutout can be found in Figure 7.27b the corresponding intensity values can be found in Figure 7.27c.\n",
      "To simplify the setting lets consider it our goal to discern the foreground and background pixels in an image based on their intensity values assuming foreground pixels have a high intensity value and background pixels a low intensity. In that way we create a binary partition of the image labeling all values above the intensity threshold as belonging to foreground pixels and all values below the intensity threshold as belonging to background pixels. In that sense thresholding converts a grayscale image to a binary image.\n",
      "252 CHAPTER 8. IMAGESEGMENTATION aImage of the town of Aalborg Denmark source Walter Daems bEdgepixelsderivedusingtheCannyedgedetec tion algorithm white  background black  fore ground cAccumulatorcell matrix dDetected lines indicated on the original image Figure 8.10 Illustration of the accumulatorcell algorithm on a grayscale test image University of Antwerp  TI\n",
      "This calls for a new type of color model. One that does not base itself on a particular choice of red green or blue but that is device independent .\n",
      "One might argue as to whether image analysis systems are more intelligent than image recognition systems. Actually it depends on the application at hand. Sometimes object recognition is required before one can deduce propertiescharacteristics of those objects.\n",
      "Every specific tristimulus vector corresponds to a specific color perception. That said it must be noted that not all tristimulus vectors correspond to valid perceptions. For example the tristimulus values of the vectors located on the individual axes for red green or blue will never occur in our brain. Its not a matter of not seeing them. It is a matter that they do not DIP20233.9TB Digital Image Processing  Text book\n",
      "To continue a bit further on the last issue regarding modes of movement without help it is impossible to make your eyeballs perform a perfect circular trajectory. Our tendency to move our eyes in a saccadic way ruins every attempt. Try doing so while a friend of yours watches your eyes. Heshe will clearly see the saccadic motions. On the contrary Move your finger in a big vertical circle in front of you and follow it with your eyes. Your friend will be able to see a continuous eye movement. Therefore it can be concluded that it is not some muscularly limitation but just one of the peculiarities of the big eye movement controller our brain. It is trained in the very first months of our life and in a Darwinian way naturally selected over the past millions of years to create still images of the things we are interested in. When man evolved from hunting behavior to a more sedentary behavior no doubt we lost some of our servomotoric capabilities.\n",
      "Uniform vs. graded quantization If all intervals are equal in size we are using socalled uniform quantization . Using intervals that are not of equal size is called nonuniform or graded quantization . Alaw or law quantization employed in cellphone communications are examples of the latter.\n",
      "We consider the value of the pixel normalized to the range 0to1as a membership function describing the pixels degree of membership to the foreground. We call these normalized intensity functions intensity membership functions . The higher the value the more the pixel belongs to the foreground. A white pixel with a value of 1 will be fully member of the foreground. The lower the intensity value the less it belongs to the foreground. A black pixel with a value of 0 will be in no way member of the foreground.\n",
      "7.1 Introduction Morphology is the science of shapes and forms. It exists in many science domains. In biology morphology is the science of the composition of living organisms. In linguistics it is the science of the composition of words and sentences. In the field of digital image processing it is the science of describing how images are composed using basic elements or objects. It is the science of deciding what pixels belong to the background what pixels belong to the foreground and what interesting objects can be enumerated on the foreground.\n",
      "Remarks This is typically done by finding corresponding control points in the two images followed by the quest for a transformation that relates the corresponding control points.\n",
      "Still it is important to realize that color is not a physical concept. It is our impression of the effect waves with different wavelengths have on the cones in our eyes. This makes color a difficult and partly an artificial concept. Our eyes cannot measure wavelength. Color is how our brain translates wavelengths in sensations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canny Edge detection is an algorithm used in image processing to detect a wide range of edges in images. It involves multiple steps such as gradient calculation, non-maximum suppression, and edge tracking by hysteresis.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "question = input(\"> \")\n",
    "answer = chatbot.askQuestion(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
